{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch入门实战（7）：基于BERT实现简单的中文文本摘要任务（Summarization task）"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iioSnail/chaotic-transformer-tutorials/blob/master/bert_summarization_demo.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 如果你没有使用Google Drive，请不要运行这个代码块\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m      3\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# 如果你没有使用Google Drive，请不要运行这个代码块\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 本文涉及知识点\n",
    "\n",
    "1. [nn.Transformer的使用](https://blog.csdn.net/zhaohongfei_358/article/details/126019181)\n",
    "2. [Transformer源码解读](https://blog.csdn.net/zhaohongfei_358/article/details/126085246) (了解即可)\n",
    "3. [Pytorch中DataLoader和Dataset的基本用法](https://blog.csdn.net/zhaohongfei_358/article/details/122742656)\n",
    "4. [Masked-Attention的机制和原理](https://blog.csdn.net/zhaohongfei_358/article/details/125858248)\n",
    "5. [Pytorch自定义损失函数](https://blog.csdn.net/zhaohongfei_358/article/details/125759911)\n",
    "6. [Hugging Face快速入门](https://blog.csdn.net/zhaohongfei_358/article/details/126224199)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 本文内容\n",
    "\n",
    "本文将使用Hugging Face提供的Bert模型和数据集进行迁移学习，完成购物评论的中文文本摘要任务（Summarization Task）。最终效果为：\n",
    "\n",
    "原评论：本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。\n",
    "摘要后：此书不是本人购买\n",
    "\n",
    "Hugging Face虽然提供了Summarization任务很方便的迁移学习API，但本文并不会使用。为了更好的知识学习和泛化，本文将会采用一种更为通用的方式来模型构造和模型训练，所以本文采用的为基础的中文bert模型`bert-base-chinese`。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 环境配置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本文重点依赖Hugging Face的两个重要类库datasets和transformers，所以需要安装：\n",
    "\n",
    "```\n",
    "transformers==4.21\n",
    "datasets==2.4\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入本文要使用的所有依赖包:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 用于加载hugging face数据集\n",
    "from datasets import load_dataset\n",
    "# 用于加载bert-base-chinese模型的分词器\n",
    "from transformers import AutoTokenizer\n",
    "# 用于加载bert-base-chinese模型\n",
    "from transformers import AutoModel\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 全局配置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义一些全局变量，我是不太喜欢一些全局变量在函数中传来传去的，太麻烦了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# 文本（评论）的最大长度\n",
    "text_max_length = 512\n",
    "# 摘要的最大长度\n",
    "summary_max_length = 48\n",
    "epochs = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 每多少步，打印一次loss\n",
    "log_per_step = 1\n",
    "# 每多少步存储一次模型\n",
    "save_per_step = 5000\n",
    "\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"./drive/MyDrive/model/transformer_checkpoints\")\n",
    "# 如果工作目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "\n",
    "print(\"Device:\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加载数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在Hugging Face中找了一圈，最终锁定了一个叫`amazon_reviews_multi`的数据集（[链接](https://huggingface.co/datasets/amazon_reviews_multi/viewer/zh/train)）：\n",
    "\n",
    "<img src=\"./images/hf_6.png\" width=\"1000\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本次由于是做文本摘要，所以只需要review_body作为评论内容和review_title作为摘要内容。\n",
    "\n",
    "我们先来加载一下数据集："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (C:\\Users\\zhaohongfei1\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\zh\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d9f124d93c44ad1899b9523a50dd068"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"amazon_reviews_multi\", \"zh\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载成功后，来看一下内容："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n        num_rows: 200000\n    })\n    validation: Dataset({\n        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n        num_rows: 5000\n    })\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到该数据集提供了train/validation/test三份，为了简单起见，我们不使用validation和test数据集。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset And Dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载好数据集后，我们就可以开始构建Dataset了，我们这里Dataset就是返回评论和其摘要："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(SummarizationDataset, self).__init__()\n",
    "        # 拿到对应的数据\n",
    "        self.dataset = dataset[mode]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 取第index条\n",
    "        data = self.dataset[index]\n",
    "        # 取其评论\n",
    "        text = data['review_body']\n",
    "        # 取对应的摘要\n",
    "        summary = data['review_title']\n",
    "        # 返回\n",
    "        return text, summary\n",
    "\n",
    "    def __len__(self):\n",
    "        return 16\n",
    "        # return len(self.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataset = SummarizationDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来打印看一下；"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "('本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。',\n '此书不是本人购买')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构造好Dataset后，就可以来构造Dataloader了。在构造Dataloader前，我们需要先定义好分词器："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来尝试使用一下分词器："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 2769, 3633, 1762, 2110,  739, 3918, 2428, 2110,  739,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"我正在学习深度学习\", return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以正常运行。其中101表示“开始”(`[CLS]`)，102表示句子结束(`[SEP]`)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们接着构造我们的Dataloader。我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('评论', '摘要'), ('评论', '摘要'), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             tgt和tgt_y与src格式一样\n",
    "             n_tokens为本轮预测时有效token数\n",
    "    \"\"\"\n",
    "    text, summary = zip(*batch)\n",
    "    text, summary = list(text), list(summary)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "    tgt = tokenizer(summary, padding='max_length', max_length=summary_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    tgt_y = {}\n",
    "    for key, value in tgt.items():\n",
    "        tgt_y[key] = value[:, 1:]\n",
    "\n",
    "    for key, value in tgt.items():\n",
    "        tgt[key] = value[:, :-1]\n",
    "\n",
    "    n_tokens = tgt_y['attention_mask'].sum().item()\n",
    "\n",
    "    return src, tgt, tgt_y, n_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来看一眼train_loader的数据："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 3119, 1168,  ...,    0,    0,    0],\n        [ 101, 7490, 2094,  ...,    0,    0,    0],\n        [ 101, 7509, 5056,  ...,    0,    0,    0],\n        ...,\n        [ 101, 4263, 6564,  ...,    0,    0,    0],\n        [ 101, 6579,  743,  ...,    0,    0,    0],\n        [ 101,  741, 7027,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我们使用bert作为encoder，然后使用`nn.TransformerDecoder`作为Decoder，然后再加上最后一个预测层组成完整的模型，如下图所示：\n",
    "\n",
    "<img src=\"./images/bert_transformer.png\" width=\"400\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class SummarizationModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SummarizationModel, self).__init__()\n",
    "\n",
    "        # 加载bert模型\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        # 定义Decoder层\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=768, nhead=8, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "\n",
    "        # 从bert中把embedding层提取出来，因为pytorch的Decoder中不包含embedding部分\n",
    "        self.embeddings = self.bert.embeddings\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Linear(768, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        前向传播，获取decoder的输出。注意是decoder的输出，不是最后线性层的输出\n",
    "        :param src: 分词后的评论数据\n",
    "        :param tgt: 前面累计预测出的结果\n",
    "        :return: decoder的输出\n",
    "        \"\"\"\n",
    "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
    "        # 得到encoder的输出\n",
    "        last_hidden_state = self.bert(**src).last_hidden_state\n",
    "        # 构造tgt_mask，就是那个阶梯形状的mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt['input_ids'].size(-1)).to(device)\n",
    "        # 构造key_padding_mask，用于mask非句子成分\n",
    "        tgt_key_padding_mask = tgt['attention_mask'] == 0\n",
    "        # 将tgt的tensor提取出来，作为decoder的输入。\n",
    "        decoder_inputs = self.embeddings(tgt['input_ids'])\n",
    "        # 将encoder的输出和tgt作为decoder的输入传入decoder，得到输出\n",
    "        decoder_outputs = self.decoder(tgt=decoder_inputs, memory=last_hidden_state, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        return decoder_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SummarizationModel()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义好模型后，我们来定义一下损失函数，由于需要稍微做一些特殊处理，所以我单独写了个类："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class SummarizationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SummarizationLoss, self).__init__()\n",
    "        # 使用经典的多分类CrossEntropyLoss作为损失函数，忽略index=0的，因为他们是填充\n",
    "        self.criteria = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def forward(self, outputs, tgt_y, n_tokens):\n",
    "        \"\"\"\n",
    "        损失函数的前向传递\n",
    "        :param outputs: 最终预测层的输出。例如，Shape为(64, 47, 768)，表示64个句子，每个句子47个词，每个词768维\n",
    "        :param tgt_y: Label。例如，Shape为(64, 47)，表示64个句子，每个句子47个词\n",
    "        :param n_tokens: 有效词的数量（非填充词的数量）。例如，1283表示在这64*47个词中，有1283个有效词\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        # 由于有多个句子构成，每个句子有多个词，所以flatten一下，将Shape变成(64*47)\n",
    "        targets = tgt_y['input_ids'].flatten()\n",
    "        # outputs同理，将前面两个维度合并\n",
    "        outputs = outputs.view(-1, tokenizer.vocab_size)\n",
    "        # 计算损失，然后正则化一下，就是平均一下，平均到每个词上的损失\n",
    "        return self.criteria(outputs, targets) / n_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来开始正式训练模型，首先定义出损失函数和优化器："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "criteria = SummarizationLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "开始训练："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "\n",
    "# 清空一下cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 定义几个变量，帮助打印loss\n",
    "total_loss = 0.\n",
    "# 记录步数\n",
    "step = 0\n",
    "\n",
    "# 由于src，tgt都是字典类型的，定义一个辅助函数帮助to(device)\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    for i, (src, tgt, tgt_y, n_tokens) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        src, tgt, tgt_y = to_device(src), to_device(tgt), to_device(tgt_y)\n",
    "        # 传入模型进行前向传递\n",
    "        outputs = model(src, tgt)\n",
    "        # 将decoder的输出送给predictor进行预测\n",
    "        outputs = model.predictor(outputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs, tgt_y, n_tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "        if step % save_per_step == 0:\n",
    "            torch.save(model, model_dir / f\"model_{step}.pt\")\n",
    "\n",
    "        del src, tgt, tgt_y, outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Step: 0/1, total loss:0.0530\n",
      "Epoch 2/1000, Step: 0/1, total loss:0.0446\n",
      "Epoch 3/1000, Step: 0/1, total loss:0.0415\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型使用"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将模型调成推理模式\n",
    "device = 'cpu'\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    \"\"\"\n",
    "    模型推理，输入为评论，输出为摘要\n",
    "    :param text: 一个长评论\n",
    "    :return: 短摘要\n",
    "    \"\"\"\n",
    "    # 对长评论进行分词\n",
    "    src = tokenizer(text, return_tensors='pt')\n",
    "    # 构造[CLS]，即开始标志\n",
    "    tgt = {\n",
    "        'input_ids': torch.LongTensor([[101]]), # 101为开始标志\n",
    "        'attention_mask': torch.LongTensor([[1]]),\n",
    "    }\n",
    "    src, tgt = to_device(src), to_device(tgt)\n",
    "\n",
    "    # 循环反复调用模型进行推理，直到达到摘要最大长度或遇到结束(102)标志\n",
    "    for i in range(summary_max_length):\n",
    "        outputs = model(src, tgt)\n",
    "        index = model.predictor(outputs[:, -1, :]).argmax()\n",
    "        tgt['input_ids'] = torch.concat([tgt['input_ids'], index.view(1, -1)], dim=1)\n",
    "        tgt['attention_mask'] = torch.concat([tgt['attention_mask'], torch.LongTensor([[1]])], dim=1)\n",
    "\n",
    "        if index == 102:\n",
    "            break\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tgt['input_ids'].squeeze())\n",
    "    return ''.join(tokens).replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}