{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BERT源码实现与解读(Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 本文内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[论文地址](https://arxiv.org/abs/1810.04805):  https://arxiv.org/abs/1810.04805\n",
    "\n",
    "BERT原论文中也没有太多关于BERT的细节，如果不想看原文，可以直接参考我的笔记[BERT论文阅读](https://blog.csdn.net/zhaohongfei_358/article/details/126838417)\n",
    "\n",
    "在BERT的论文中，描述的基本的都是模型如何训练等，对于本身的模型架构并没有过多的说明。这也可以理解，因为BERT架构本身也确实比较简单，就是一些TransformerEncoder的堆叠。虽然这么说，但不看代码很多人还是无法具体知道BERT是怎么样的，所以本文就来搭建一个BERT模型，并使用论文中提到的MLM任务和NSP任务对模型进行训练。\n",
    "\n",
    "本篇需要大家有Transformer的基础，默认你已经熟悉Transformer，所以本篇会直接使用Pytorch中的`nn.Transformer`进行实现。\n",
    "\n",
    "本篇参考了[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)的部分代码，如果感兴趣，可以看看。\n",
    "\n",
    "相关文章：\n",
    "\n",
    "[Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解](https://blog.csdn.net/zhaohongfei_358/article/details/126019181): https://blog.csdn.net/zhaohongfei_358/article/details/126019181\n",
    "\n",
    "[万字逐行解析与实现Transformer，并进行德译英实战](https://blog.csdn.net/zhaohongfei_358/article/details/126085246): https://blog.csdn.net/zhaohongfei_358/article/details/126085246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "导入本文需要的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cpu'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.1'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BERT模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "原始BERT使用了两种任务对BERT进行预训练，所以我们将数据集和训练放在后面，先把BERT模型定义出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在Transformer中，对token的编码使用的是token embedding+position embedding，而在BERT中，增加了segment embedding，即文本的段落信息，在原论文中，bert的inputs是两句话，该embedding用于区分这是第一句话还是第二句话。我们先将这3中Embedding定义出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Token Embedding就是一个`nn.Emebdding`，和Transformer一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Position Embedding和Transformer也一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Segment Embedding也是一个nn.Embedding，但需要注意的是其词典大小只有3，其中0是填充，1代表第一句话，2代表第二句话。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义完上面三个Embedding类，就可以把BERT Embedding类定义出来了，其比较简单，就是它们三个相加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: token的词典大小\n",
    "        :param embed_size: 词向量大小\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: 词典大小\n",
    "        :param hidden: 隐状态大小，即词向量大小\n",
    "        :param n_layers: TransformerEncoderLayer的层数\n",
    "        :param attn_heads: Multi-head Attention的head数\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # 论文中提到它们使用的feed_forward_hidden的大小为hidde_size*4\n",
    "        feed_forward_hidden = hidden * 4\n",
    "\n",
    "        # 定义BERT的embedding\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n",
    "\n",
    "        # 在论文中提到，BERT中Transformer的激活函数使用的是GELU\n",
    "        transformer_encoder = nn.TransformerEncoderLayer(d_model=hidden, nhead=attn_heads, dim_feedforward=feed_forward_hidden, dropout=dropout, activation=F.gelu, batch_first=True)\n",
    "\n",
    "        # 多层TransformerEncoder堆叠\n",
    "        self.transformer_blocks = nn.ModuleList([copy.deepcopy(transformer_encoder) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        \"\"\"\n",
    "        BERT前向传递\n",
    "        :param x: 要被bert编码的向量，例如[[1,2,3,4,5,5,4,3,2,1,0,0]]，\n",
    "                  即一对儿句子，包含有两句话(1,2,3,4,5)和(5,4,3,2,1),其中0是填充\n",
    "        :param segment_info: 句子的段落信息，例如[1,1,1,1,1,2,2,2,2,2,0,0]，\n",
    "                             即前5个token属于第一句话，接下来5个token是第二句话，\n",
    "                             0是填充，不属于任何话。\n",
    "        :return: 所有token经过bert后包含上下文的隐状态，例如Shape为(1, 12, 768)，\n",
    "                 即1个句子，12个token，每个token被编码成了768维的向量\n",
    "        \"\"\"\n",
    "\n",
    "        # 定义key_padding_mask，\n",
    "        # 例如句子：<bos>唱跳Rap篮球<eos><pad><pad>\n",
    "        # 对应的key_padding_mask为：[T, T, T, T, T, T, False, False]\n",
    "        # 最后两个False是对<pad>进行mask\n",
    "        key_padding_mask = x <= 0\n",
    "\n",
    "        # 将index编码成向量\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # 将编码后的向量经过TransformerEncoder一层一层传递\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义好BERT模型类的代码后，我们来初始化一下。在原论文中，作者提出了两种不同大小的BERT，分别为：\n",
    "\n",
    "- $\\bf{BERT_{BASE}}$：12层，hidden size为768，Attention Head数为12，共110M个参数。\n",
    "- $\\bf{BERT_{LARGE}}$：24层，hidden size为1024，Attention Head数为16，共340M个参数。\n",
    "\n",
    "我们接下来就将两种BERT定义出来（词向量是参考了Hugging Face上bert_base_uncased的bert模型）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base参数量:  108497664\n",
      "bert_large参数量:  333566976\n"
     ]
    }
   ],
   "source": [
    "bert_base = BERT(vocab_size=30522, hidden=768, n_layers=12, attn_heads=12)\n",
    "bert_large = BERT(vocab_size=30522, hidden=1024, n_layers=24, attn_heads=16)\n",
    "print(\"bert_base参数量: \", sum([param.nelement() for param in bert_base.parameters()]))\n",
    "print(\"bert_large参数量: \", sum([param.nelement() for param in bert_large.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "可以看到，bert_base和bert_large的参数量与原文的描述基本一致。接下来简单尝试使用一下BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base outputs size: torch.Size([1, 12, 768])\n",
      "bert_large outputs size: torch.Size([1, 12, 1024])\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([[1,2,3,4,5,5,4,3,2,1,0,0]])\n",
    "segment_info = torch.LongTensor([[1,1,1,1,1,2,2,2,2,2,0,0]])\n",
    "print(\"bert_base outputs size:\", bert_base(x, segment_info).size())\n",
    "print(\"bert_large outputs size:\", bert_large(x, segment_info).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 预训练BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在BERT原论文中，作者使用了两种任务对BERT进行预训练，分别是MLM任务(masked language model)和NSP(Next Sentence Prediction)，我们这里也使用这两种任务对BERT进行简单的预训练。\n",
    "\n",
    "**MLM任务(masked language model)**:\n",
    "\n",
    "MLM任务简介：MLM任务就是把一个句子中的部分token给替换掉，然后让bert去结合上下文来预测被替换掉的词是什么。\n",
    "\n",
    "在BERT原论文中，作者是将句子中15%的token给替换掉，而对于被替换的15%的token使用的规则为：选择80%的token使用`[MASK]`这个特殊的token进行替换，10%的token随机替换成其他token，10%不做任何变化。\n",
    "\n",
    "这里我们简单一点，对一个句子只选择2个token进行替换，并且只替换成`[MASK]`\n",
    "\n",
    "**Next Sentence Prediction(NSP)任务**：\n",
    "\n",
    "NSP任务简介：NSP任务就是预测传给BERT的两句话是不是一对儿，是一个二分类任务。预测方式就是使用输入的第一个token`[CLS]`的输出接全连接网络进行二分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在开始前，我们先准备一下词典，这里就简单的弄几个词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"大家好，我是练习时长两年半的个人练习生蔡徐坤，喜欢唱跳RAP篮球，接下来我会为大家带来一首鸡你太美。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(sentence, specials=['[PAD]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来定义数据集，这里我只定义两条数据，仅仅是为了模拟BERT训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BERT的输入，以[CLS]开头，两句话中间以[SEP]分割，长度都为24。\n",
    "inputs = [\n",
    "    '[CLS] 我 是 [MASK] 习 时 长 两 年 半 [SEP] 的 个 人 练 习 生 [MASK] 徐 坤 [SEP] [PAD] [PAD] [PAD]',\n",
    "    '[CLS] 喜 欢 [MASK] 跳 R A P 篮 [MASK] [SEP] 鸡 你 太 [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'\n",
    "]\n",
    "\n",
    "# 段落信息，表示该token属于哪句话\n",
    "segment_label = [\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,0,0,0,0,0,0,0,0]\n",
    "]\n",
    "\n",
    "# 定义MLM任务的targets，对于[CLS]和[SEP]不需要预测，所以标签中用[PAD]代替\n",
    "mlm_targets = [\n",
    "    '[PAD] 我 是 练 习 时 长 两 年 半 [PAD] 的 个 人 练 习 生 蔡 徐 坤 [PAD] [PAD] [PAD] [PAD]',\n",
    "    '[PAD] 喜 欢 唱 跳 R A P 篮 球 [PAD] 鸡 你 太 美 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'\n",
    "]\n",
    "\n",
    "# 定义NSP任务的targets\n",
    "nsp_targets = [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来将上面的文本数据变成token，并将其变成tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([2, 24])\n",
      "targets.shape: torch.Size([2, 24])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.LongTensor([vocab(input.split(' ')) for input in inputs])\n",
    "segment_label = torch.LongTensor(segment_label)\n",
    "mlm_targets = torch.LongTensor([vocab(target.split(' ')) for target in mlm_targets])\n",
    "nsp_targets = torch.LongTensor(nsp_targets)\n",
    "print(\"inputs.shape:\", inputs.size())\n",
    "print(\"targets.shape:\", mlm_targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义好数据集后，我们需要定义出针对MLM和NSP任务的网络，其实就是后面再接一个全连接层就行了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BERTLM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # 这里就使用bert_base吧\n",
    "        self.bert = BERT(vocab_size=vocab_size, hidden=768, n_layers=12, attn_heads=12)\n",
    "\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsp_outputs shape: torch.Size([2, 2])\n",
      "mlm_outputs shape: torch.Size([2, 24, 46])\n"
     ]
    }
   ],
   "source": [
    "bert_mlm = BERTLM(len(vocab))\n",
    "nsp_outputs, mlm_outputs = bert_mlm(inputs, segment_label)\n",
    "print(\"nsp_outputs shape:\", nsp_outputs.size())\n",
    "print(\"mlm_outputs shape:\", mlm_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来开始训练网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(bert_mlm.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4.49, nsp loss: 0.4757, mlm_loss 4.014\n",
      "loss 3.851, nsp loss: 0.007476, mlm_loss 3.843\n",
      "loss 3.67, nsp loss: 0.002525, mlm_loss 3.667\n",
      "loss 3.597, nsp loss: 0.002195, mlm_loss 3.595\n",
      "loss 3.497, nsp loss: 0.002728, mlm_loss 3.494\n",
      "loss 3.459, nsp loss: 0.002472, mlm_loss 3.457\n",
      "loss 3.408, nsp loss: 0.003018, mlm_loss 3.404\n",
      "loss 3.381, nsp loss: 0.003101, mlm_loss 3.378\n",
      "loss 3.311, nsp loss: 0.00601, mlm_loss 3.305\n",
      "loss 3.228, nsp loss: 0.00369, mlm_loss 3.225\n",
      "loss 3.204, nsp loss: 0.006772, mlm_loss 3.197\n",
      "loss 3.187, nsp loss: 0.004654, mlm_loss 3.183\n",
      "loss 3.069, nsp loss: 0.003755, mlm_loss 3.066\n",
      "loss 3.023, nsp loss: 0.003315, mlm_loss 3.019\n",
      "loss 2.957, nsp loss: 0.003419, mlm_loss 2.953\n",
      "loss 2.845, nsp loss: 0.007694, mlm_loss 2.837\n",
      "loss 2.857, nsp loss: 0.007342, mlm_loss 2.85\n",
      "loss 2.821, nsp loss: 0.01467, mlm_loss 2.807\n",
      "loss 2.766, nsp loss: 0.009931, mlm_loss 2.757\n",
      "loss 2.764, nsp loss: 0.01004, mlm_loss 2.754\n",
      "loss 2.756, nsp loss: 0.004404, mlm_loss 2.752\n",
      "loss 2.708, nsp loss: 0.002927, mlm_loss 2.705\n",
      "loss 2.754, nsp loss: 0.00252, mlm_loss 2.751\n",
      "loss 2.71, nsp loss: 0.003143, mlm_loss 2.707\n",
      "loss 2.715, nsp loss: 0.002215, mlm_loss 2.713\n",
      "loss 2.691, nsp loss: 0.002162, mlm_loss 2.689\n",
      "loss 2.621, nsp loss: 0.001629, mlm_loss 2.619\n",
      "loss 2.647, nsp loss: 0.001219, mlm_loss 2.646\n",
      "loss 2.627, nsp loss: 0.003697, mlm_loss 2.623\n",
      "loss 2.812, nsp loss: 0.004737, mlm_loss 2.807\n",
      "loss 2.666, nsp loss: 0.005748, mlm_loss 2.66\n",
      "loss 2.714, nsp loss: 0.01657, mlm_loss 2.697\n",
      "loss 2.717, nsp loss: 0.02108, mlm_loss 2.696\n",
      "loss 2.704, nsp loss: 0.01768, mlm_loss 2.686\n",
      "loss 2.697, nsp loss: 0.009418, mlm_loss 2.687\n",
      "loss 2.682, nsp loss: 0.006151, mlm_loss 2.676\n",
      "loss 2.688, nsp loss: 0.004699, mlm_loss 2.683\n",
      "loss 2.682, nsp loss: 0.003566, mlm_loss 2.678\n",
      "loss 2.619, nsp loss: 0.002988, mlm_loss 2.616\n",
      "loss 2.68, nsp loss: 0.001556, mlm_loss 2.678\n",
      "loss 2.645, nsp loss: 0.001237, mlm_loss 2.644\n",
      "loss 2.654, nsp loss: 0.001753, mlm_loss 2.653\n",
      "loss 2.64, nsp loss: 0.0006957, mlm_loss 2.64\n",
      "loss 2.617, nsp loss: 0.0008831, mlm_loss 2.616\n",
      "loss 2.61, nsp loss: 0.0007202, mlm_loss 2.609\n",
      "loss 2.627, nsp loss: 0.0005216, mlm_loss 2.627\n",
      "loss 2.602, nsp loss: 0.00059, mlm_loss 2.601\n",
      "loss 2.581, nsp loss: 0.0004983, mlm_loss 2.581\n",
      "loss 2.548, nsp loss: 0.0004479, mlm_loss 2.548\n",
      "loss 2.561, nsp loss: 0.000679, mlm_loss 2.56\n",
      "loss 2.502, nsp loss: 0.0004481, mlm_loss 2.501\n",
      "loss 2.486, nsp loss: 0.0005349, mlm_loss 2.485\n",
      "loss 2.517, nsp loss: 0.000347, mlm_loss 2.517\n",
      "loss 2.41, nsp loss: 0.0003713, mlm_loss 2.41\n",
      "loss 2.395, nsp loss: 0.0003342, mlm_loss 2.395\n",
      "loss 2.324, nsp loss: 0.0005006, mlm_loss 2.323\n",
      "loss 2.253, nsp loss: 0.0008445, mlm_loss 2.252\n",
      "loss 2.189, nsp loss: 0.000712, mlm_loss 2.188\n",
      "loss 2.133, nsp loss: 0.001751, mlm_loss 2.132\n",
      "loss 1.985, nsp loss: 0.002085, mlm_loss 1.983\n",
      "loss 1.872, nsp loss: 0.0135, mlm_loss 1.858\n",
      "loss 1.591, nsp loss: 0.007061, mlm_loss 1.584\n",
      "loss 1.446, nsp loss: 0.001156, mlm_loss 1.445\n",
      "loss 1.267, nsp loss: 0.005965, mlm_loss 1.261\n",
      "loss 1.203, nsp loss: 0.005106, mlm_loss 1.198\n",
      "loss 1.131, nsp loss: 0.01704, mlm_loss 1.114\n",
      "loss 1.006, nsp loss: 0.0007063, mlm_loss 1.005\n",
      "loss 0.8604, nsp loss: 0.0005704, mlm_loss 0.8598\n",
      "loss 0.8771, nsp loss: 0.01159, mlm_loss 0.8655\n",
      "loss 0.7953, nsp loss: 0.0008441, mlm_loss 0.7945\n",
      "loss 0.6927, nsp loss: 0.002725, mlm_loss 0.69\n",
      "loss 0.7357, nsp loss: 0.001429, mlm_loss 0.7343\n",
      "loss 0.7091, nsp loss: 0.0068, mlm_loss 0.7023\n",
      "loss 0.542, nsp loss: 0.00273, mlm_loss 0.5392\n",
      "loss 0.4856, nsp loss: 0.00602, mlm_loss 0.4795\n",
      "loss 0.4678, nsp loss: 0.002863, mlm_loss 0.4649\n",
      "loss 0.4386, nsp loss: 0.00134, mlm_loss 0.4373\n",
      "loss 0.4714, nsp loss: 0.003999, mlm_loss 0.4674\n",
      "loss 0.3809, nsp loss: 0.001943, mlm_loss 0.379\n",
      "loss 0.3174, nsp loss: 0.006497, mlm_loss 0.3109\n",
      "loss 0.2951, nsp loss: 0.001601, mlm_loss 0.2935\n",
      "loss 0.2894, nsp loss: 0.0009814, mlm_loss 0.2884\n",
      "loss 0.269, nsp loss: 0.001594, mlm_loss 0.2674\n",
      "loss 0.2501, nsp loss: 0.001229, mlm_loss 0.2489\n",
      "loss 0.2252, nsp loss: 0.0004571, mlm_loss 0.2247\n",
      "loss 0.2275, nsp loss: 0.00175, mlm_loss 0.2258\n",
      "loss 0.212, nsp loss: 0.0007395, mlm_loss 0.2113\n",
      "loss 0.2104, nsp loss: 0.001159, mlm_loss 0.2092\n",
      "loss 0.1816, nsp loss: 0.0004789, mlm_loss 0.1812\n",
      "loss 0.1768, nsp loss: 0.001188, mlm_loss 0.1756\n",
      "loss 0.1803, nsp loss: 0.002111, mlm_loss 0.1782\n",
      "loss 0.1654, nsp loss: 0.0007102, mlm_loss 0.1646\n",
      "loss 0.1481, nsp loss: 0.000708, mlm_loss 0.1474\n",
      "loss 0.1583, nsp loss: 0.0008034, mlm_loss 0.1575\n",
      "loss 0.1252, nsp loss: 0.0002821, mlm_loss 0.1249\n",
      "loss 0.1357, nsp loss: 0.001079, mlm_loss 0.1346\n",
      "loss 0.1269, nsp loss: 0.0003893, mlm_loss 0.1265\n",
      "loss 0.1033, nsp loss: 0.0007439, mlm_loss 0.1025\n",
      "loss 0.1107, nsp loss: 0.0004162, mlm_loss 0.1102\n",
      "loss 0.1073, nsp loss: 0.000742, mlm_loss 0.1066\n",
      "loss 0.1069, nsp loss: 0.001086, mlm_loss 0.1058\n",
      "loss 0.1158, nsp loss: 0.0008994, mlm_loss 0.1149\n",
      "loss 0.1003, nsp loss: 0.0006228, mlm_loss 0.09964\n",
      "loss 0.08813, nsp loss: 0.0005053, mlm_loss 0.08763\n",
      "loss 0.09276, nsp loss: 0.0004154, mlm_loss 0.09234\n",
      "loss 0.08812, nsp loss: 0.0001731, mlm_loss 0.08794\n",
      "loss 0.08907, nsp loss: 0.0002249, mlm_loss 0.08884\n",
      "loss 0.06842, nsp loss: 0.0001752, mlm_loss 0.06825\n",
      "loss 0.07488, nsp loss: 0.000423, mlm_loss 0.07446\n",
      "loss 0.07353, nsp loss: 0.0003509, mlm_loss 0.07318\n",
      "loss 0.06162, nsp loss: 0.0006601, mlm_loss 0.06096\n",
      "loss 0.06037, nsp loss: 0.0001426, mlm_loss 0.06023\n",
      "loss 0.06379, nsp loss: 0.0002464, mlm_loss 0.06354\n",
      "loss 0.04247, nsp loss: 6.294e-05, mlm_loss 0.04241\n",
      "loss 0.04904, nsp loss: 0.0001333, mlm_loss 0.04891\n",
      "loss 0.05485, nsp loss: 0.0003744, mlm_loss 0.05448\n",
      "loss 0.04583, nsp loss: 0.0002311, mlm_loss 0.0456\n",
      "loss 0.035, nsp loss: 0.0001712, mlm_loss 0.03483\n",
      "loss 0.03382, nsp loss: 0.0001329, mlm_loss 0.03369\n",
      "loss 0.02937, nsp loss: 0.0002775, mlm_loss 0.0291\n",
      "loss 0.03895, nsp loss: 0.0002913, mlm_loss 0.03866\n",
      "loss 0.04726, nsp loss: 0.0003719, mlm_loss 0.04689\n",
      "loss 0.0263, nsp loss: 0.0003093, mlm_loss 0.02599\n",
      "loss 0.03534, nsp loss: 0.0003004, mlm_loss 0.03504\n",
      "loss 0.02788, nsp loss: 0.0004243, mlm_loss 0.02745\n",
      "loss 0.02758, nsp loss: 0.0002028, mlm_loss 0.02738\n",
      "loss 0.02832, nsp loss: 0.000241, mlm_loss 0.02808\n",
      "loss 0.01905, nsp loss: 0.0003305, mlm_loss 0.01872\n",
      "loss 0.03207, nsp loss: 0.0008309, mlm_loss 0.03124\n",
      "loss 0.02589, nsp loss: 0.0003299, mlm_loss 0.02556\n",
      "loss 0.02583, nsp loss: 0.0001618, mlm_loss 0.02567\n",
      "loss 0.02417, nsp loss: 0.000247, mlm_loss 0.02393\n",
      "loss 0.01991, nsp loss: 0.0004237, mlm_loss 0.01949\n",
      "loss 0.0272, nsp loss: 0.0004075, mlm_loss 0.02679\n",
      "loss 0.01876, nsp loss: 0.0002179, mlm_loss 0.01855\n",
      "loss 0.0203, nsp loss: 0.0001343, mlm_loss 0.02016\n",
      "loss 0.02239, nsp loss: 0.0002722, mlm_loss 0.02212\n",
      "loss 0.01768, nsp loss: 0.0002283, mlm_loss 0.01746\n",
      "loss 0.01941, nsp loss: 0.0002116, mlm_loss 0.0192\n",
      "loss 0.01483, nsp loss: 8.714e-05, mlm_loss 0.01474\n",
      "loss 0.01311, nsp loss: 0.0001334, mlm_loss 0.01297\n",
      "loss 0.02087, nsp loss: 0.0002913, mlm_loss 0.02058\n",
      "loss 0.02082, nsp loss: 0.0001719, mlm_loss 0.02065\n",
      "loss 0.01429, nsp loss: 0.0003081, mlm_loss 0.01398\n",
      "loss 0.01883, nsp loss: 0.0001801, mlm_loss 0.01865\n",
      "loss 0.01378, nsp loss: 0.0002484, mlm_loss 0.01353\n",
      "loss 0.015, nsp loss: 0.0003124, mlm_loss 0.01468\n",
      "loss 0.01396, nsp loss: 0.0005878, mlm_loss 0.01337\n",
      "loss 0.01567, nsp loss: 0.0002115, mlm_loss 0.01546\n",
      "loss 0.0117, nsp loss: 0.0001671, mlm_loss 0.01154\n",
      "loss 0.01448, nsp loss: 0.0002372, mlm_loss 0.01424\n",
      "loss 0.01581, nsp loss: 0.0006137, mlm_loss 0.01519\n",
      "loss 0.01385, nsp loss: 0.0002008, mlm_loss 0.01365\n",
      "loss 0.01552, nsp loss: 6.902e-05, mlm_loss 0.01545\n",
      "loss 0.01633, nsp loss: 0.0001632, mlm_loss 0.01617\n",
      "loss 0.0143, nsp loss: 0.0001215, mlm_loss 0.01418\n",
      "loss 0.009968, nsp loss: 8.154e-05, mlm_loss 0.009887\n",
      "loss 0.0111, nsp loss: 9.298e-05, mlm_loss 0.01101\n",
      "loss 0.01393, nsp loss: 6.842e-05, mlm_loss 0.01386\n",
      "loss 0.01179, nsp loss: 0.0001123, mlm_loss 0.01168\n",
      "loss 0.009844, nsp loss: 9.584e-05, mlm_loss 0.009748\n",
      "loss 0.01524, nsp loss: 0.0001818, mlm_loss 0.01506\n",
      "loss 0.00956, nsp loss: 6.354e-05, mlm_loss 0.009496\n",
      "loss 0.01184, nsp loss: 6.795e-05, mlm_loss 0.01177\n",
      "loss 0.009849, nsp loss: 5.781e-05, mlm_loss 0.009792\n",
      "loss 0.009618, nsp loss: 8.559e-05, mlm_loss 0.009532\n",
      "loss 0.01173, nsp loss: 0.0001525, mlm_loss 0.01158\n",
      "loss 0.01349, nsp loss: 0.0001084, mlm_loss 0.01338\n",
      "loss 0.009323, nsp loss: 6.234e-05, mlm_loss 0.009261\n",
      "loss 0.01036, nsp loss: 0.0001314, mlm_loss 0.01023\n",
      "loss 0.009061, nsp loss: 0.0001823, mlm_loss 0.008878\n",
      "loss 0.01039, nsp loss: 0.0004148, mlm_loss 0.009975\n",
      "loss 0.01174, nsp loss: 0.0001423, mlm_loss 0.01159\n",
      "loss 0.009517, nsp loss: 0.0003012, mlm_loss 0.009216\n",
      "loss 0.01008, nsp loss: 0.0001464, mlm_loss 0.009935\n",
      "loss 0.009186, nsp loss: 0.0002983, mlm_loss 0.008888\n",
      "loss 0.0101, nsp loss: 0.0001045, mlm_loss 0.009997\n",
      "loss 0.009918, nsp loss: 0.0001609, mlm_loss 0.009757\n",
      "loss 0.009168, nsp loss: 0.0001389, mlm_loss 0.009029\n",
      "loss 0.008802, nsp loss: 0.0001405, mlm_loss 0.008661\n",
      "loss 0.008072, nsp loss: 0.0001247, mlm_loss 0.007948\n",
      "loss 0.009261, nsp loss: 9.989e-05, mlm_loss 0.009162\n",
      "loss 0.009667, nsp loss: 0.0001374, mlm_loss 0.00953\n",
      "loss 0.008987, nsp loss: 0.0001088, mlm_loss 0.008878\n",
      "loss 0.00888, nsp loss: 0.000126, mlm_loss 0.008754\n",
      "loss 0.009106, nsp loss: 5.627e-05, mlm_loss 0.009049\n",
      "loss 0.01019, nsp loss: 7.379e-05, mlm_loss 0.01011\n",
      "loss 0.008008, nsp loss: 0.0001846, mlm_loss 0.007823\n",
      "loss 0.008457, nsp loss: 0.0001159, mlm_loss 0.008342\n",
      "loss 0.008294, nsp loss: 0.0001639, mlm_loss 0.00813\n",
      "loss 0.008365, nsp loss: 0.0002505, mlm_loss 0.008114\n",
      "loss 0.007582, nsp loss: 0.0001323, mlm_loss 0.00745\n",
      "loss 0.01027, nsp loss: 0.0001511, mlm_loss 0.01012\n",
      "loss 0.006925, nsp loss: 0.0001701, mlm_loss 0.006754\n",
      "loss 0.00892, nsp loss: 0.0001098, mlm_loss 0.00881\n",
      "loss 0.007216, nsp loss: 0.0001198, mlm_loss 0.007096\n",
      "loss 0.009759, nsp loss: 0.0001194, mlm_loss 0.00964\n",
      "loss 0.006729, nsp loss: 7.736e-05, mlm_loss 0.006652\n",
      "loss 0.00754, nsp loss: 0.0001457, mlm_loss 0.007394\n",
      "loss 0.007507, nsp loss: 0.0001491, mlm_loss 0.007358\n",
      "loss 0.007871, nsp loss: 9.083e-05, mlm_loss 0.00778\n",
      "loss 0.008193, nsp loss: 0.0001634, mlm_loss 0.008029\n",
      "loss 0.006821, nsp loss: 0.0001051, mlm_loss 0.006716\n",
      "loss 0.008192, nsp loss: 0.000147, mlm_loss 0.008045\n",
      "loss 0.008025, nsp loss: 0.0001602, mlm_loss 0.007865\n",
      "loss 0.006859, nsp loss: 0.0001918, mlm_loss 0.006667\n",
      "loss 0.00868, nsp loss: 0.0001348, mlm_loss 0.008545\n",
      "loss 0.006929, nsp loss: 6.878e-05, mlm_loss 0.00686\n",
      "loss 0.008753, nsp loss: 9.191e-05, mlm_loss 0.008661\n",
      "loss 0.007456, nsp loss: 0.0001064, mlm_loss 0.007349\n",
      "loss 0.008345, nsp loss: 3.374e-05, mlm_loss 0.008311\n",
      "loss 0.007745, nsp loss: 5.984e-05, mlm_loss 0.007685\n",
      "loss 0.007765, nsp loss: 2.944e-05, mlm_loss 0.007736\n",
      "loss 0.007764, nsp loss: 4.315e-05, mlm_loss 0.007721\n",
      "loss 0.007126, nsp loss: 7.093e-05, mlm_loss 0.007055\n",
      "loss 0.006596, nsp loss: 4.434e-05, mlm_loss 0.006552\n",
      "loss 0.006665, nsp loss: 0.0001068, mlm_loss 0.006558\n",
      "loss 0.006723, nsp loss: 6.771e-05, mlm_loss 0.006656\n",
      "loss 0.006432, nsp loss: 0.0001066, mlm_loss 0.006325\n",
      "loss 0.007628, nsp loss: 0.000156, mlm_loss 0.007472\n",
      "loss 0.009031, nsp loss: 0.0001173, mlm_loss 0.008913\n",
      "loss 0.00596, nsp loss: 8.44e-05, mlm_loss 0.005875\n",
      "loss 0.006105, nsp loss: 4.339e-05, mlm_loss 0.006062\n",
      "loss 0.00531, nsp loss: 4.589e-05, mlm_loss 0.005264\n",
      "loss 0.006454, nsp loss: 8.583e-05, mlm_loss 0.006368\n",
      "loss 0.006101, nsp loss: 9.906e-05, mlm_loss 0.006002\n",
      "loss 0.005633, nsp loss: 0.0001136, mlm_loss 0.00552\n",
      "loss 0.008009, nsp loss: 0.0001411, mlm_loss 0.007867\n",
      "loss 0.005498, nsp loss: 0.000101, mlm_loss 0.005397\n",
      "loss 0.005033, nsp loss: 3.958e-05, mlm_loss 0.004994\n",
      "loss 0.006316, nsp loss: 9.238e-05, mlm_loss 0.006224\n",
      "loss 0.00614, nsp loss: 0.0001197, mlm_loss 0.00602\n",
      "loss 0.006396, nsp loss: 5.698e-05, mlm_loss 0.006339\n",
      "loss 0.007277, nsp loss: 5.483e-05, mlm_loss 0.007222\n",
      "loss 0.006156, nsp loss: 0.0001154, mlm_loss 0.006041\n",
      "loss 0.006306, nsp loss: 5.257e-05, mlm_loss 0.006254\n",
      "loss 0.005321, nsp loss: 5.09e-05, mlm_loss 0.00527\n",
      "loss 0.005898, nsp loss: 3.672e-05, mlm_loss 0.005861\n",
      "loss 0.006083, nsp loss: 3.946e-05, mlm_loss 0.006044\n",
      "loss 0.005404, nsp loss: 9.131e-05, mlm_loss 0.005313\n",
      "loss 0.005565, nsp loss: 6.401e-05, mlm_loss 0.005501\n",
      "loss 0.006188, nsp loss: 0.0001011, mlm_loss 0.006087\n",
      "loss 0.006467, nsp loss: 7.915e-05, mlm_loss 0.006387\n",
      "loss 0.007051, nsp loss: 6.258e-05, mlm_loss 0.006989\n",
      "loss 0.005854, nsp loss: 0.0001031, mlm_loss 0.005751\n",
      "loss 0.006643, nsp loss: 9.501e-05, mlm_loss 0.006548\n",
      "loss 0.005677, nsp loss: 5.531e-05, mlm_loss 0.005622\n",
      "loss 0.004667, nsp loss: 6.199e-05, mlm_loss 0.004605\n",
      "loss 0.005953, nsp loss: 5.15e-05, mlm_loss 0.005901\n",
      "loss 0.005154, nsp loss: 0.0001625, mlm_loss 0.004991\n",
      "loss 0.005072, nsp loss: 4.935e-05, mlm_loss 0.005023\n",
      "loss 0.005659, nsp loss: 7.212e-05, mlm_loss 0.005586\n",
      "loss 0.005432, nsp loss: 9.644e-05, mlm_loss 0.005336\n",
      "loss 0.00557, nsp loss: 0.0001072, mlm_loss 0.005463\n",
      "loss 0.004638, nsp loss: 5.209e-05, mlm_loss 0.004586\n",
      "loss 0.007378, nsp loss: 5.901e-05, mlm_loss 0.007319\n",
      "loss 0.004379, nsp loss: 6.473e-05, mlm_loss 0.004314\n",
      "loss 0.004789, nsp loss: 6.234e-05, mlm_loss 0.004727\n",
      "loss 0.00647, nsp loss: 3.457e-05, mlm_loss 0.006435\n",
      "loss 0.00643, nsp loss: 0.0001298, mlm_loss 0.0063\n",
      "loss 0.005355, nsp loss: 7.272e-05, mlm_loss 0.005283\n",
      "loss 0.005425, nsp loss: 4.244e-05, mlm_loss 0.005383\n",
      "loss 0.004892, nsp loss: 4.828e-05, mlm_loss 0.004843\n",
      "loss 0.00503, nsp loss: 6.437e-05, mlm_loss 0.004966\n",
      "loss 0.006988, nsp loss: 6.354e-05, mlm_loss 0.006925\n",
      "loss 0.00543, nsp loss: 6.878e-05, mlm_loss 0.005361\n",
      "loss 0.005484, nsp loss: 0.0001091, mlm_loss 0.005375\n",
      "loss 0.005338, nsp loss: 4.721e-05, mlm_loss 0.005291\n",
      "loss 0.004728, nsp loss: 0.0001067, mlm_loss 0.004621\n",
      "loss 0.00433, nsp loss: 5.305e-05, mlm_loss 0.004277\n",
      "loss 0.005535, nsp loss: 5.889e-05, mlm_loss 0.005476\n",
      "loss 0.005663, nsp loss: 0.0001182, mlm_loss 0.005545\n",
      "loss 0.005463, nsp loss: 5.233e-05, mlm_loss 0.005411\n",
      "loss 0.004579, nsp loss: 7.474e-05, mlm_loss 0.004505\n",
      "loss 0.005045, nsp loss: 8.94e-05, mlm_loss 0.004955\n",
      "loss 0.006256, nsp loss: 3.755e-05, mlm_loss 0.006218\n",
      "loss 0.004526, nsp loss: 0.0001163, mlm_loss 0.00441\n",
      "loss 0.00404, nsp loss: 9.417e-05, mlm_loss 0.003946\n",
      "loss 0.005309, nsp loss: 4.196e-05, mlm_loss 0.005267\n",
      "loss 0.004605, nsp loss: 6.974e-05, mlm_loss 0.004535\n",
      "loss 0.004698, nsp loss: 5.972e-05, mlm_loss 0.004639\n",
      "loss 0.005542, nsp loss: 5.984e-05, mlm_loss 0.005482\n",
      "loss 0.005145, nsp loss: 9.93e-05, mlm_loss 0.005046\n",
      "loss 0.004784, nsp loss: 0.0001007, mlm_loss 0.004683\n",
      "loss 0.005184, nsp loss: 7.581e-05, mlm_loss 0.005108\n",
      "loss 0.004115, nsp loss: 6.354e-05, mlm_loss 0.004051\n",
      "loss 0.004728, nsp loss: 4.578e-05, mlm_loss 0.004682\n",
      "loss 0.003969, nsp loss: 3.85e-05, mlm_loss 0.00393\n",
      "loss 0.004119, nsp loss: 0.0001522, mlm_loss 0.003967\n",
      "loss 0.004745, nsp loss: 9.512e-05, mlm_loss 0.00465\n",
      "loss 0.005024, nsp loss: 9.393e-05, mlm_loss 0.00493\n",
      "loss 0.004373, nsp loss: 7.057e-05, mlm_loss 0.004302\n",
      "loss 0.004158, nsp loss: 9.191e-05, mlm_loss 0.004067\n",
      "loss 0.004286, nsp loss: 0.000123, mlm_loss 0.004163\n",
      "loss 0.004184, nsp loss: 0.0001097, mlm_loss 0.004075\n",
      "loss 0.004778, nsp loss: 7.498e-05, mlm_loss 0.004703\n",
      "loss 0.004133, nsp loss: 0.000164, mlm_loss 0.003969\n",
      "loss 0.004519, nsp loss: 7.391e-05, mlm_loss 0.004445\n",
      "loss 0.004217, nsp loss: 4.911e-05, mlm_loss 0.004168\n",
      "loss 0.004457, nsp loss: 8.523e-05, mlm_loss 0.004372\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    nsp_outputs, mlm_outputs = bert_mlm(inputs, (inputs>0).int())\n",
    "    nsp_loss = criterion(nsp_outputs, nsp_targets)\n",
    "    mlm_loss = criterion(mlm_outputs.view(-1, 46), mlm_targets.view(-1))\n",
    "    loss = nsp_loss + mlm_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss {:.4}, nsp loss: {:.4}, mlm_loss {:.4}\".format(loss, nsp_loss, mlm_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> 这里虽然只有一个样本，但收敛的并不是很快，因为网络太深了，如果你把BERT改浅一点，就会发现收敛特别快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "训练好之后，我们来试一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = '我 是 练 习 时 长 两 年 半 的 个 人 练 习 生 [MASK] 徐 坤'\n",
    "inputs = torch.LongTensor([vocab(inputs.split(' '))])\n",
    "segment_label = torch.ones(inputs.size()).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nsp_outputs, mlm_outputs = bert_mlm(inputs, segment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '是', '练', '习', '时', '长', '两', '年', '半', '的', '个', '人', '练', '习', '生', '蔡', '徐', '坤']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.lookup_tokens(mlm_outputs.argmax(-1)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 参考资料\n",
    "\n",
    "[(原论文)BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805): https://arxiv.org/abs/1810.04805\n",
    "\n",
    "[(论文阅读)BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://blog.csdn.net/zhaohongfei_358/article/details/126838417): https://blog.csdn.net/zhaohongfei_358/article/details/126838417\n",
    "\n",
    "[Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解](https://blog.csdn.net/zhaohongfei_358/article/details/126019181): https://blog.csdn.net/zhaohongfei_358/article/details/126019181\n",
    "\n",
    "[万字逐行解析与实现Transformer，并进行德译英实战](https://blog.csdn.net/zhaohongfei_358/article/details/126085246): https://blog.csdn.net/zhaohongfei_358/article/details/126085246\n",
    "\n",
    "[BERT-Pytorch实现](https://github.com/codertimo/BERT-pytorch): https://github.com/codertimo/BERT-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}